# 基于 jieba 分词相同词库

实现基本的分词功能。

实现 DFA 算法，依赖简体中文词库。

实现分词。

## 分词初步，只关心词语组

利用 DFA 算法匹配结果

## 内存优化

默认的分词词库为 34W+，采用 DFA 实现，还是非常占用内存的。

如何实现不加载 dict 而获取其中内容？

nio

mapped file

## 结果的选择

初期可以简单点，直接选择最后一个（贪心匹配作为结果。）

利用 DAG+分治算法处理结果+频率

那也只是针对最后的结果进行处理的一种选择策略而已。

### 截断

可以保留每一个截断的结果。

```
中文最棒
```

可以切割为：

```
中文最棒
中文 最棒
中文 最 棒
中 文 最 棒
```

## 后期关心频率

根据分治算法，选择频率更高的可能词组。

## 其他标注

英文

数字

特殊符号

停顿词

人名

地名

标点符号

颜文字 ^_^

emoji

# 词库格式

```
单词 词频 词性
苹果 3 n
```

## 对于字典的兼容性

默认词频=3

词性=un 未知

如果用户没有这些值的话。

## 词性的枚举。

Done.